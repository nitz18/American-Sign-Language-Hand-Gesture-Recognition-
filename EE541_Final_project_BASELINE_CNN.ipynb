{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rc0Ai_mA-5_j"
   },
   "source": [
    "## Baseline CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYs_BPsrZbQB"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets --upgrade --quiet # To download the opendatasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8K75l7K-j_K5"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "\n",
    "import torch.utils.data as td\n",
    "import random, time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as tt\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import models\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rt7pEFf0kCUi",
    "outputId": "fef288ed-6ed1-4b58-c759-1a3e182ba322"
   },
   "outputs": [],
   "source": [
    "dataset_url = 'https://www.kaggle.com/grassknoted/asl-alphabet'\n",
    "\n",
    "if os.path.exists('./asl-alphabet'):\n",
    "  shutil.rmtree('./asl-alphabet')\n",
    "\n",
    "od.download(dataset_url) # Enter the username, and the kaggle public API key to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnaldJvN6OVM"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = './asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(128), # resize the image to the size 128\n",
    "    transforms.RandomCrop(128), # apply random cropping\n",
    "    transforms.Grayscale(1), # converting the image to grayscale\n",
    "    transforms.ToTensor(), # converting the PIL Image object to a torch tensor\n",
    "    transforms.Normalize((0.485), (0.229)) # normalizing the image\n",
    "    ])\n",
    "\n",
    "train_data_l = datasets.ImageFolder(root = TRAIN_DATA_PATH, transform=TRANSFORM_IMG) # Load the dataset, \n",
    "                                                                    # and apply the transformation\n",
    "\n",
    "n_train_examples = int(len(train_data_l) * 0.8)\n",
    "n_valid_examples = len(train_data_l) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data_l,\n",
    "                                           [n_train_examples, n_valid_examples])\n",
    "\n",
    "# use test transform for validation\n",
    "valid_data = copy.deepcopy(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VvYz9IHJea4"
   },
   "outputs": [],
   "source": [
    "class_list = train_data_l.classes # List of all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVjVOzxz7IpN"
   },
   "outputs": [],
   "source": [
    "TEST_DATA_PATH = \"/content/asl-alphabet/asl_alphabet_test\"\n",
    "\n",
    "TRANSFORM_IMG_TEST = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.RandomCrop(128),\n",
    "    transforms.Grayscale(1), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485), (0.229))\n",
    "    ])\n",
    "\n",
    "test_data_l = datasets.ImageFolder(root = TEST_DATA_PATH, transform=TRANSFORM_IMG_TEST)\n",
    "\n",
    "test_labels = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','nothing','space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VLgTqoi7mK4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "test_iterator = data.DataLoader(test_data_l, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcEvg58NvhRV"
   },
   "source": [
    "Check the number of samples in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZZDBS2Qc-09",
    "outputId": "8017c2d6-f557-4c65-d736-e18cbcc3df73"
   },
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJUZTMavyz9"
   },
   "source": [
    "Define a function to plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7-zAcWgnbc5"
   },
   "outputs": [],
   "source": [
    "def plot_images(images, labels, classes, normalize=False):\n",
    "\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "\n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image_min = image.min()\n",
    "            image_max = image.max()\n",
    "            image.clamp_(min=image_min, max=image_max)\n",
    "            image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy()[:,:,0],cmap='gray')\n",
    "        ax.set_title(classes[labels[i]])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "QQNJHVeincWJ",
    "outputId": "91e3cf5c-a434-4786-bc09-8adc7477c9d5"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 30\n",
    "\n",
    "images, labels = zip(*[(image, label) for image, label in\n",
    "                       [train_data[i] for i in range(N_IMAGES)]])\n",
    "\n",
    "classes = train_data_l.classes\n",
    "\n",
    "plot_images(images, labels, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNzro8q3pNw7"
   },
   "source": [
    "Define the model using the architecture of CNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zY6_Jx1VeGJd"
   },
   "outputs": [],
   "source": [
    "output_dim = 29\n",
    "\n",
    "class Net(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            nn.Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Defining another 2D convolution layer\n",
    "            nn.Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(4 * 32 * 32, output_dim)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UowXhuDepwic"
   },
   "source": [
    "Define the batch size and the iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uut5pci3eErZ",
    "outputId": "a772c715-ff37-41be-a67f-6b97ca8f94f9"
   },
   "outputs": [],
   "source": [
    "train_iterator = data.DataLoader(train_data,\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data,\n",
    "                                 batch_size=BATCH_SIZE)\n",
    "\n",
    "OUTPUT_DIM = len(train_data_l.classes) \n",
    "\n",
    "model = Net() # Defining the model\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "model.apply(initialize_parameters) # Initialzing the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P6CJEAbe9wP"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Adam optimizer with Learning rate of 0.001\n",
    "criterion = nn.CrossEntropyLoss() # Cross entropy loss for calculating the loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # finding what device to run the model on\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTZY0YTjs2lN"
   },
   "source": [
    "Define a function to calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7SA-2nEfQEc"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyuMZZPXtgxD"
   },
   "source": [
    "Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6eZzBOM_fYqz"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # turn off the dropout during evaluation time\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_6uP5plfjRt"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9MiADyNfnvP"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "list_training_acc = []\n",
    "list_val_acc = []\n",
    "list_training_loss = []\n",
    "list_val_loss = []\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "\n",
    "    list_training_acc.append(train_acc)\n",
    "    list_val_acc.append(valid_acc)\n",
    "\n",
    "    list_training_loss.append(train_loss)\n",
    "    list_val_loss.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'baseline_cnn-model.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x0fFLdCsZ0Q"
   },
   "outputs": [],
   "source": [
    "plt.plot(list_training_acc)\n",
    "plt.plot(list_val_acc)\n",
    "plt.legend([\"Training Acc\", 'Validation Acc'])\n",
    "plt.title('Baseline CNN Accuracy performance')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list_training_loss)\n",
    "plt.plot(list_val_loss)\n",
    "plt.legend([\"Training Acc\", 'Validation Acc'])\n",
    "plt.title('Baseline CNN Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyr-RIgvu5Ti"
   },
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhjQobMZmv0T"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/baseline_cnn-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5j8Yx9fYotCw"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, _), gt in zip(iterator, test_labels):\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            y_prob = F.softmax(y_pred, dim=-1)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim=0)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "\n",
    "    return images, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOLi_WMBoxFJ"
   },
   "outputs": [],
   "source": [
    "images, probs = get_predictions(model, test_iterator, device)\n",
    "pred_labels = torch.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQTGx-T8DN1G"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred_labels_f = []\n",
    "\n",
    "for l in range(len(pred_labels)):\n",
    "    pred_labels_f.append(class_list[pred_labels[l]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4tvGu5aDkGk"
   },
   "outputs": [],
   "source": [
    "def acc(true,pred):\n",
    "    assert len(true) == len(pred)\n",
    "    cnt=0\n",
    "    for i,j in zip(true,pred):\n",
    "        if i==j:\n",
    "            cnt+=1\n",
    "    return cnt/len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oK1uQsKEm-r",
    "outputId": "5b5e0951-ca34-4abc-e26e-b6274e607476"
   },
   "outputs": [],
   "source": [
    "acc(pred_labels_f, test_labels) # Test Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yg-mG3Qx76Sy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EE541_Final_project_BASELINE_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
